# @package _global_

defaults:
  - base
  - override /model: retriever
  - override /model/head: colbert # todo colbert
  - override /model/bert: pubmed # todo: pubmed
  - override /callbacks:
      - checkpoint
      - progress_bar
  - override /datamodule/index_builder: colbert # todo colbert

base:
  seed: null
  target_metric: validation/retriever/top10_Accuracy
  target_mode: max
  is_search: false

trainer:
  gpus: 8
  strategy: dp
  min_epochs: 1
  max_epochs: 200
  accumulate_grad_batches: 2
  check_val_every_n_epoch: 1
  # reload_dataloaders_every_n_epochs: 1


model:
  lr: 1e-5
  weight_decay: 1e-3
  num_warmup_steps: 1000
  bert:
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.3

datamodule:

  # how often to map the dataset
  mapping_freq: null

  # dataloader
  train_batch_size: 16
  eval_batch_size: 16
  num_workers: 8 # setting this too high might lead `Dataset.map()` to hang
  persistent_workers: false
  pin_memory: true
  drop_last: true # todo: fix this

  # dataset builder
  use_subset: false
  num_proc: 8
  output_columns:
    - question.input_ids
    - question.attention_mask
    - document.input_ids
    - document.attention_mask
    - document.row_idx
    - document.match_score
    - document.retrieval_score

  # documents
  n_documents:
    train: 20
    validation: 100
    test: 100
  max_pos_docs: 1
  filter_unmatched: true
  select_mode: sample

  # tag documents as positive or negative
  relevance_classifier:
    interpretable: false

  # OpenQA builder parameters
  builder:
    batch_size: 100
    writer_batch_size: 10000
    dataset_builder:
      n_query_tokens: 10
    corpus_builder:
      use_subset: false

  # Index parameters
  index_builder:
    loader_kwargs:
      batch_size: 1000
      num_workers: 8
      pin_memory: true

logger:
  wandb:
    group: colbert-qa-retriever
