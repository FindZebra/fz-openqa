# @package _global_

defaults:
  - base
  - override /model: option_retriever
  - override /model/bert: pubmed
  - override /model/head: colbert
  - override /datamodule/builder: concat_openqa
  - override /datamodule/index_builder: ${model/head}
  - override /datamodule/sampler: boost_positives
  - override /datamodule/transform: option_dropout # todo: run [:F]
  - override /datamodule/dataset_filter: none
  - override /datamodule/builder/analytics:
      - retriever_accuracy
      - sequence_lengths
      - retriever_distribution
  - override /logger: wandb
  - override /callbacks:
      - checkpoint
      - lr_monitor

base:
  seed: 2789492787  # todo: remove [run:A]
  target_metric: validation/reader/Accuracy
  target_mode: max
  is_search: false
  exp_info: "Experimenting with OptionDropout + DPR: this runs uses `update_freq=5` (prev. value=15)."

trainer:
  gpus: 8
  strategy: dp
  min_epochs: 1
  max_epochs: 1_000
  gradient_clip_val: 0.5
  accumulate_grad_batches: 4
  val_check_interval: 0.5

model:
  lr: 1e-5
  weight_decay: 1e-3
  num_warmup_steps: 1_000
  num_training_steps: 50_000
  bert:
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1 # todo: updated [run:B] reverted: [run:C]
  module:
    resample_k: null
    alpha: 0
    max_batch_size: 200 # todo: try increasing this
    eval_topk: 30
    grad_expr: in_batch
    share_heads: false # todo: update
  head:
    downscaling: 1 # todo: changed from 2
    kernel_size: null # todo: changed from 9
    bias: true # todo: update
    use_mask: false # todo: update
    output_size: 32 # todo: update

datamodule:
  # how often to map the dataset
  dataset_update:
    freq: 5 # todo: run [:G]
    reset_optimizer: false # todo: test `false`
    index_first_epoch: false
    builder_args:
      relevance_classifier: null
      # batch_size: 1000  # todo: remove [run:A]

  # dataloader
  train_batch_size: 16 # todo: run [:F]
  eval_batch_size: 64
  num_workers: 16
  pin_memory: true

  # dataset builder
  use_subset: false
  corpus_subset: false
  num_proc: 8
  output_columns:
    - answer.target
    - question.input_ids
    - question.attention_mask
    - document.input_ids
    - document.attention_mask
    - document.row_idx
    - document.match_score
    - document.retrieval_score
    - document.retrieval_rank

  # documents: retrieval & sampling
  n_retrieved_documents: 1_000
  n_documents:
    train: 10
    validation: 10
    test: 10

  # tag documents as positive or negative
  relevance_classifier:
    interpretable: false

  # document sampler
  sampler:
    total: ${datamodule.n_documents}
    temperature: 1
    largest:
      train: false
      validation: true
      test: true

  # OpenQA builder parameters
  builder:
    batch_size: 100
    writer_batch_size: 1_000
    dataset_builder:
      dset_name: medqa-us
      question_length: null # todo: experiment with this
      n_query_tokens: 10
    corpus_builder:
      use_subset: ${datamodule.corpus_subset}
      passage_length: 200
      passage_stride: 100 # todo: updated [:D] - reverted

  # Index parameters
  index_builder:
    model_output_keys: [ _hq_retriever_, _hd_retriever_ ]
    keep_maxsim_on_cpu: false
    faiss_train_size: 1_000_000
    p: 100
    dtype: float32  # todo: updated [run:B]
    max_chunksize: null
    maxsim_chunksize: 4_000 # todo: updated [run:B]
    persist_cache: false
    faiss_args:
      factory: IVF100,PQ16x8
      metric_type: 0 # == faiss.METRIC_INNER_PRODUCT
      nprobe: 16
    loader_kwargs:
      batch_size: 4_000
      num_workers: 16
      pin_memory: true


logger:
  wandb:
    group: option-retriever
