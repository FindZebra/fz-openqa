# @package _global_

defaults:
  - base
  - override /model: option_retriever
  - override /model/bert: pubmed
  - override /model/module/retriever_head: colbert
  - override /model/module/reader_head: colbert
  - override /model/module/gradients: reinforce
  - override /datamodule/builder: concat_openqa
  - override /datamodule/index_builder: ${model/module/retriever_head}
  - override /datamodule/sampler: priority
  - override /datamodule/transform: none
  - override /datamodule/relevance_classifier: exact_match
  - override /datamodule/score_transform: none
  - override /datamodule/dataset_transform: none
  - override /datamodule/builder/analytics:
      - retriever_accuracy
      - sequence_lengths
      - retriever_distribution
      - log_retrieved_documents
  - override /logger: wandb
  - override /callbacks:
      - checkpoint
      - lr_monitor
      - log_predictions
      - progress_bar
      - viz_maxsim

base:
  seed: null # TODO: remove
  target_metric: validation/reader/Accuracy
  target_mode: max
  exp_info: "--"
  batch_size: 32
  device_batch_size: 1
  eval_device_batch_size: 8
  infer_batch_mul: 100

trainer:
  gradient_clip_val: 0.5 # 2.0 might be unstable
  accumulate_grad_batches: ${base.accumulate_grad_batches}
  val_check_interval: 1.0
  precision: 32
  max_steps: 100_000

model:
  # global model configuration
  metric_size: 64
  metric_type: inner_product
  num_warmup_steps: 5_000

  # bert configuration
  bert:
    config:
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1

  # optimizer configuration
  ema_decay: null
  num_lr_warmup_steps: 1_000 # Colbert uses no warmup
  num_training_steps: 20_000 # TODO !!
  optimizer: adamw
  optimizer_params:
    lr: 1e-5
    weight_decay: 1e-3
    # correct_bias: false # todo: remove and use torch's AdamW
    eps: 1e-8 # as in the Colbert repo

  # model parameter, passed to the model's `forward()` and `step()` methods
  parameters:
    reader_kl_weight: 0.5
    alpha:
      mode: cosine
      num_steps: ${model.num_warmup_steps}
      initial_value: 1e-2
      final_value: 1
    proposal_kl_weight: 0
    retriever_kl_weight: 0
    tau: null
    maxim_retriever_kl_weight: null
    maxim_reader_kl_weight: null
    agg_retriever_kl_weight: 0

  # Module definition
  module:
    mask_punctuation: true # MaxSim focuses too much on punctuation
    split_bert_layers: 0
    resample_k: null
    alpha: 0
    max_batch_size: 200
    share_documents_across_batch: false
    retriever_head:
      output_size: ${model.metric_size}
      metric_type: ${model.metric_type}
      id: retriever
      use_soft_score: false
      compute_agg_score: true
      normalize: true
      scale_init: 0.1
      share_parameters: true
      bias: false
    reader_head:
      output_size: ${model.metric_size}
      metric_type: ${model.metric_type}
      id: reader
      use_soft_score: true
      normalize: false
      scale_init: 0.1
      share_parameters: true
      bias: false
    gradients:
      use_baseline: false # todo
      expr: A2


datamodule:
  # how often to map the dataset
  dataset_update:
    freq: ${int_mul:2,${model.num_warmup_steps}}
    reset_optimizer: true # TODO !!
    test_every_update: false # might cause a bug with checkpointing
    builder_args:
      # relevance_classifier: null
      score_transform: null

  # dataloader
  train_batch_size: ${base.step_batch_size}
  eval_batch_size: ${int_mul:${base.eval_device_batch_size},${base.n_devices}}
  num_workers: 12
  pin_memory: true

  # dataset builder
  dset_name: medqa-us
  corpus_name: medqa
  use_subset: false
  corpus_subset: false
  num_proc: 8
  output_columns:
    - answer.target
    - question.row_idx
    - question.input_ids
    - question.attention_mask
    - question.token_type_ids
    - document.input_ids
    - document.attention_mask
    - document.row_idx
    - document.match_score
    - document.proposal_score
    - document.proposal_rank

  # documents: retrieval & sampling
  n_retrieved_documents: 100
  n_documents: 10

  # document sampler
  sampler:
    total: ${datamodule.n_documents}
    temperature: 1
    largest:
      train: false
      validation: false
      test: false

  # OpenQA builder parameters
  builder:
    batch_size: 100
    writer_batch_size: 1_000
    dataset_builder:
      dset_name: ${datamodule.dset_name}
      query_expansion: null
      n_query_tokens: 1 # todo!!! (10)
      n_answer_tokens: 1 # previous exps: 10
      max_length: 350 # todo!!!
    corpus_builder:
      dset_name: ${datamodule.corpus_name}
      use_subset: ${datamodule.corpus_subset}
      passage_length: 200
      passage_stride: 100
      append_document_title: true

  # Index parameters
  index_builder:
    es_temperature: 10. # todo !!!
    auxiliary_weight: 4. # todo: 10
    model_output_keys: [ _hq_, _hd_ ]
    p: 1024
    dtype: float16
    max_chunksize: null
    maxsim_chunksize: 8_000 # use 4_000 for metric_size 128
    persist_cache: false
    index_factory: IVF100k,PQ32x8
    handler: flat
    keep_faiss_on_cpu: false
    train_faiss_on_cpu: false
    fais_gpu_ratio: 0.5
    faiss_train_size: null
    n_ranking_workers: ${datamodule.num_workers}
    nprobe: 32
    shard_faiss: false
    metric_type: ${model.metric_type}
    loader_kwargs:
      batch_size: ${int_mul:${base.n_devices},${base.infer_batch_mul},${base.device_batch_size}}
      num_workers: ${datamodule.num_workers}
      pin_memory: true


logger:
  wandb:
    group: option-retriever
