# @package _global_

defaults:
  - base
  - override /model: option_retriever
  - override /model/bert: pubmed
  - override /model/module/retriever_head: colbert
  - override /model/module/reader_head: colbert
  - override /model/module/gradients: reinforce
  - override /datamodule/builder: concat_openqa
  - override /datamodule/index_builder: ${model/module/retriever_head}
  - override /datamodule/sampler: priority
  - override /datamodule/transform: none
  - override /datamodule/relevance_classifier: exact_match
  - override /datamodule/score_transform: none
  - override /datamodule/dataset_filter: none
  - override /datamodule/builder/analytics:
      - retriever_accuracy
      - sequence_lengths
      - retriever_distribution
      - log_retrieved_documents
  - override /logger: wandb
  - override /callbacks:
      - checkpoint
      - lr_monitor
      - log_predictions
      - progress_bar

base:
  seed: null # TODO: remove
  target_metric: validation/reader/Accuracy
  target_mode: max
  is_search: false
  exp_info: "Colbert + Bayesian head"
  batch_size: 32
  device_batch_size: 1
  eval_device_batch_size: 8
  n_devices: ${int_max:1,${trainer.gpus}}
  step_batch_size: ${int_mul:${base.device_batch_size},${base.n_devices}}
  accumulate_grad_batches: ${int_div:${base.batch_size},${base.step_batch_size}}
  sharing_strategy: file_system
  git_hash: ${git_hash:}
  git_hash_short: ${git_hash_short:}

trainer:
  gpus: ${n_gpus:}
  strategy: dp
  min_epochs: 1
  max_epochs: 1_000
  gradient_clip_val: 2.0 # todo: changed from 0.5
  accumulate_grad_batches: ${base.accumulate_grad_batches}
#    _target_: fz_openqa.utils.config.IntDict
#    "0": ${int_mul:${base.accumulate_grad_batches},1}
#    "60": ${int_mul:${base.accumulate_grad_batches},2}
#    "120": ${int_mul:${base.accumulate_grad_batches},3}
  val_check_interval: 1.0
  precision: 32
  # track_grad_norm: 2

model:
  ema_decay: null
  num_warmup_steps: 1_000 # Colbert uses no warmup
  num_training_steps: 20_000 # TODO !!
  hidden_size: 32
  bert:
    config:
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1

  optimizer: adamw
  optimizer_params:
    lr: 3e-6
    weight_decay: 1e-3
    # correct_bias: false # todo: remove and use torch's AdamW
    eps: 1e-8 # as in the Colbert repo
  parameters:
    gamma: 1
    question_mask: 1
    alpha: 0
    reader_kl_weight: 0.5
    retriever_kl_weight:
      mode: cosine
      num_steps: 5_000
      initial_value: 0.1
      final_value: 1e-5
  module:
    resample_k: null
    alpha: 0
    max_batch_size: 200
    retriever_head:
      output_size: ${model.hidden_size}
      use_mask: false
      use_answer_mask: false
      bayesian: false
      learn_scale: false
      scale: 1.
      share_parameters: true
      bias: false
    reader_head:
      output_size: ${model.hidden_size}
      use_mask: false
      bayesian: false
      learn_scale: false
      scale: 1.
      share_parameters: true
      bias: false
    gradients:
      use_baseline: false # todo
      expr: A2


datamodule:
  # how often to map the dataset
  dataset_update:
    freq: 10
    reset_optimizer: true # TODO !!
    test_every_update: false # might cause a bug with checkpointing
    builder_args:
      relevance_classifier: null
      score_transform: null

  # dataloader
  train_batch_size: ${base.step_batch_size}
  eval_batch_size: ${int_mul:${base.eval_device_batch_size},${base.n_devices}}
  num_workers: 12
  pin_memory: true

  # dataset builder
  use_subset: false
  corpus_subset: false
  num_proc: 8
  output_columns:
    - answer.target
    - question.input_ids
    - question.attention_mask
    - document.input_ids
    - document.attention_mask
    - document.row_idx
    - document.match_score
    - document.retrieval_score
    - document.retrieval_rank

  # documents: retrieval & sampling
  n_retrieved_documents: 300
  n_documents: 10

  # tag documents as positive or negative
  #  relevance_classifier:
  #    interpretable: false


  # transform the score based on the relevance classifier
  score_transform:
    factor: 1.0
    normalize: false
    exponentiate: false
    max_score: 1.0
    temperature: 1.0

  # document sampler
  sampler:
    total: ${datamodule.n_documents}
    temperature: 1
    largest:
      train: false
      validation: false
      test: false

  # OpenQA builder parameters
  builder:
    batch_size: 100
    writer_batch_size: 1_000
    dataset_builder:
      dset_name: medqa-us
      query_expansion: null
      n_query_tokens: 10 # todo!!!
      n_answer_tokens: 1 # previous exps: 10
      max_length: 512 # todo!!!
    corpus_builder:
      dset_name: medqa
      use_subset: ${datamodule.corpus_subset}
      passage_length: 200
      passage_stride: 100
      append_document_title: true

  # Index parameters
  index_builder:
    es_temperature: 5. # todo !!!
    auxiliary_weight: 5.
    model_output_keys: [ _hq_, _hd_ ]
    p: 10
    dtype: float16
    max_chunksize: null
    maxsim_chunksize: 16_000 # size 128: 4_000
    persist_cache: false
    index_factory: IVF100,PQ16x8
    handler: flat
    keep_faiss_on_cpu: false
    train_faiss_on_cpu: false
    nprobe: 12
    loader_kwargs:
      batch_size: ${int_mul:${base.n_devices},500,${base.device_batch_size}}
      num_workers: 12
      pin_memory: true


logger:
  wandb:
    group: option-retriever
