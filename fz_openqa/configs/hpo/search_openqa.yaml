# @package _global_

# kill all idle processes using:
# > ps aux | grep ray:: | grep -v grep | awk '{print $2}' | xargs kill -9

# delete checkpoints
# > find . -type d -path "./*" -mmin +100 -exec rm -rf {} \;

defaults:
  - override /sys: hpo-titan.yaml
  - override /space: search_2.yaml
  - override /runner: hyperopt_asha.yaml

base:
  name: search_openqa
  target_metric: validation/reader/Accuracy
  target_mode: max
  # resources
  gpus: 8
  cpus: 32

ray:
  local_mode: False
  configure_logging: False

experiment:
  experiment: open_qa.yaml
  environ: dp8-full.yaml
  corpus: none.yaml
  sys: titan.yaml
  sys.cache_dir: ${sys.cache_dir}
  sys.shared_cache_dir: ${sys.shared_cache_dir}
  base.target_metric: ${base.target_metric}
  base.target_mode: ${base.target_mode}
  seed: null
  print_config: False
  verbose: False
  ignore_warnings: True
  trainer.progress_bar_refresh_rate: 0
  trainer.checkpoint_callback: True
  logger.wandb.group: ${base.name}
  callbacks: tune.yaml
  callbacks.model_checkpoint.cleanup_threshold: 0.1
  datamodule.train_batch_size: 24
  datamodule.eval_batch_size: 512
  datamodule.num_workers: ${base.cpus}
  trainer.max_epochs: 100
  trainer.gpus: ${base.gpus}


runner:
  search_alg:
    points_to_evaluate:
      - model.hidden_size: 128
        model.lr: 0.0001
        model.weight_decay: 0.001
        model.bert.pretrained_model_name_or_path: dmis-lab/biobert-base-cased-v1.1
        model.bert.hidden_dropout_prob: 0.1
        model.bert.attention_probs_dropout_prob: 0.1
        seed: 1
      - model.hidden_size: 128
        model.lr: 0.0001
        model.weight_decay: 0.001
        model.bert.pretrained_model_name_or_path: dmis-lab/biobert-base-cased-v1.2
        model.bert.hidden_dropout_prob: 0.1
        model.bert.attention_probs_dropout_prob: 0.1
        seed: 1
