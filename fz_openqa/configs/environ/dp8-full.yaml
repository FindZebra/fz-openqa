# @package _global_

defaults:
  # - override /callbacks: none.yaml
  - override /sys: titan.yaml


trainer:
  gpus: 8
  accelerator: dp
  precision: 32 # fp16 is not compatible with most of the DTU compute GPUs..

datamodule:
  use_subset: False
  train_batch_size: 24 # use 48 for retriever_only
  eval_batch_size: 512 # use 1024 for retriever_only
  num_proc: 32
  num_workers: 32

corpus:
  eval_batch_size: 6000

# logger: null
