# @package _global_

defaults:
  # - override /callbacks: none.yaml
  - override /sys: titan.yaml


trainer:
  gpus: 8
  accelerator: ddp
  precision: 32 # fp16 is not compatible with most of the DTU compute GPUs..
  # plugins: ddp_sharded

datamodule:
  use_subset: False
  train_batch_size: 3 # use 48 for retriever_only
  eval_batch_size: 32 # use 1024 for retriever_only
  num_proc: 8
  num_workers: 4

corpus:
  eval_batch_size: 600
  num_proc: 32

# logger: null
